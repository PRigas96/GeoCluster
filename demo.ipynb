{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8072660e8d1bf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GeoCluster Demo\n",
    "\n",
    "This notebook contains a demo of the GeoCluster algorithm for a dataset of **1000 squares** (with rotation) in 2D space.\n",
    "\n",
    "The metric used is `L_inf`, where the function is adjusted to calculate the `L_inf` distance between a square object and a point.\n",
    "\n",
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86065baf8506e9a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:05:09.386053369Z",
     "start_time": "2024-01-10T12:05:05.536599344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.k_tree import Ktree\n",
    "from src.k_tree_ls import Ktree as Ktree_ls\n",
    "from src.utils.objects.squares import loadData as loadSquares\n",
    "from src.metrics import Linf_simple\n",
    "from src.utils import plot_tools as pt\n",
    "from src.utils import accuracy as acc\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f053c4d5f7275",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialise k-tree with training parameters\n",
    "\n",
    "In order to create the hierarchical clustering tree structure, we initialise a `Ktree` object which takes as parameters the training arguments for the *Clustering* and *Critic* models, as well as the arguments for the UN sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:05:09.387039580Z",
     "start_time": "2024-01-10T12:05:09.367373577Z"
    }
   },
   "outputs": [],
   "source": [
    "dim = 2  # space dimension\n",
    "\n",
    "k = 3  # number of centroids to generate in the Clustering model\n",
    "clustering_args = {\n",
    "    \"epochs\": 30,  # number of epochs\n",
    "    \"pre_processing\": 10,\n",
    "    \"number_of_centroids\": k,  # number of centroids to generate in the Clustering model\n",
    "    \"dimension\": dim,  # space dimension\n",
    "}\n",
    "n = 30\n",
    "un_args = {\n",
    "    \"N\": n,  # number of points to sample\n",
    "    \"M\": n**2 - 1,  # number of points to return\n",
    "    \"epsilon\": 0.15,  # the epsilon ball\n",
    "}\n",
    "critic_args = {\n",
    "    \"optimizer_lr\": 5e-3,  # optimiser learning rate\n",
    "    \"epochs\": 1000,  # number of epochs\n",
    "    \"width\": 200,  # width of the model's linear layers\n",
    "    \"depth\": 5,  # depth of the model's linear layers\n",
    "}\n",
    "\n",
    "threshold = 300  # if a tree node has data less than the threshold, stop division\n",
    "\n",
    "# Initialise the k-tree structure.\n",
    "ktree = Ktree_ls(\n",
    "    threshold, data2d, Linf_simple, clustering_args, un_args, critic_args, device, dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bebb802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3D line_segments\n",
    "# n = 10000\n",
    "# lim = 50000\n",
    "# var = 500\n",
    "n = 100\n",
    "lim = 500\n",
    "var = 100\n",
    "\n",
    "bbox = [0, lim, 0, lim, 0, lim]\n",
    "length_size = [0.1, var]\n",
    "theta_size = [0, 180]\n",
    "phi_size = [0, 360]\n",
    "\n",
    "\n",
    "def create_line_segments_3d(n, bbox, length_size, theta_size, phi_size):\n",
    "    line_segments = torch.zeros((n, 6))  # ls = [x0,y0,z0, l, theta, phi]\n",
    "    for i in range(n):\n",
    "        x0 = torch.rand(1) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "        y0 = torch.rand(1) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "        z0 = torch.rand(1) * (bbox[5] - bbox[4]) + bbox[4]\n",
    "        l = torch.rand(1) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "        theta = torch.rand(1) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "        theta = torch.deg2rad(theta)\n",
    "        phi = torch.rand(1) * (phi_size[1] - phi_size[0]) + phi_size[0]\n",
    "        phi = torch.deg2rad(phi)\n",
    "        line_segments[i, :] = torch.cat([x0, y0, z0, l, theta, phi])\n",
    "    return line_segments\n",
    "\n",
    "\n",
    "data = create_line_segments_3d(n, bbox, length_size, theta_size, phi_size)\n",
    "# make the points\n",
    "data_points = torch.zeros((n, 6))\n",
    "for i in range(n):\n",
    "    data_points[i, 0] = data[i, 0]\n",
    "    data_points[i, 1] = data[i, 1]\n",
    "    data_points[i, 2] = data[i, 2]\n",
    "    data_points[i, 3] = data[i, 0] + data[i, 3] * torch.sin(data[i, 4]) * torch.cos(\n",
    "        data[i, 5]\n",
    "    )\n",
    "    data_points[i, 4] = data[i, 1] + data[i, 3] * torch.sin(data[i, 4]) * torch.sin(\n",
    "        data[i, 5]\n",
    "    )\n",
    "    data_points[i, 5] = data[i, 2] + data[i, 3] * torch.cos(data[i, 4])\n",
    "data3d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d859be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot them as 3d lines using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "for i in range(n):\n",
    "    ax.plot(\n",
    "        [data_points[i, 0], data_points[i, 3]],\n",
    "        [data_points[i, 1], data_points[i, 4]],\n",
    "        [data_points[i, 2], data_points[i, 5]],\n",
    "    )\n",
    "ax.set_xlim([bbox[0], bbox[1]])\n",
    "ax.set_ylim([bbox[2], bbox[3]])\n",
    "ax.set_zlim([bbox[4], bbox[5]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc41e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# check if 2 linesegments intersect\n",
    "\n",
    "\n",
    "def check_ls_intersection_2d(ls1, ls2):\n",
    "\n",
    "    # ls are of the form [x0,y0,l,theta]\n",
    "\n",
    "    x1 = ls1[0]\n",
    "\n",
    "    y1 = ls1[1]\n",
    "    l1 = ls1[2]\n",
    "\n",
    "    theta1 = ls1[3]\n",
    "\n",
    "    x2 = ls2[0]\n",
    "\n",
    "    y2 = ls2[1]\n",
    "    l2 = ls2[2]\n",
    "\n",
    "    theta2 = ls2[3]\n",
    "\n",
    "    # check if the lines are parallel\n",
    "\n",
    "    if torch.abs(theta1 - theta2) < 1e-6:\n",
    "\n",
    "        return False\n",
    "\n",
    "    # get their distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate #n line segments inside a bbox\n",
    "n = 100\n",
    "bbox = [0, 1000, 0, 1000]  # bbox\n",
    "length_size = [0.1, 100]  # length of the line segment\n",
    "theta_size = [0, 180]  # angle of the line segment\n",
    "\n",
    "\n",
    "# line segments will be [x0,y0,l,theta]\n",
    "# sample x0,y0 inside bbox\n",
    "def create_line_segments(n, bbox, length_size, theta_size):\n",
    "    print(f\"Creating {n} line segments\")\n",
    "    x0 = torch.rand(n) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "    y0 = torch.rand(n) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "    print(f\"x0 shape: {x0.shape}\")\n",
    "    # sample l inside length_size\n",
    "    l = torch.rand(n) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "    print(f\"l shape: {l.shape}\")\n",
    "    # sample theta inside theta_size\n",
    "    theta_size = torch.rand(n) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "    print(f\"theta shape: {theta_size.shape}\")\n",
    "    theta_size = torch.deg2rad(theta_size)\n",
    "    return torch.stack([x0, y0, l, theta_size], dim=1)\n",
    "\n",
    "\n",
    "data = create_line_segments(n, bbox, length_size, theta_size)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "# first make data from embedding to datapoints\n",
    "data_points = torch.zeros(n, 4)\n",
    "data_points[:, 0] = data[:, 0]\n",
    "data_points[:, 1] = data[:, 1]\n",
    "data_points[:, 2] = data[:, 0] + data[:, 2] * torch.cos(data[:, 3])\n",
    "data_points[:, 3] = data[:, 1] + data[:, 2] * torch.sin(data[:, 3])\n",
    "print(f\"Data points shape: {data_points.shape}\")\n",
    "data2d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([data_points[:, 0], data_points[:, 2]], [data_points[:, 1], data_points[:, 3]])\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100, 4)\n",
    "a, b, c, d = (\n",
    "    x[:, 0],\n",
    "    x[:, 1],\n",
    "    x[:, 2],\n",
    "    x[:, 3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_distances(segments, centroid):\n",
    "    print(segments.shape)\n",
    "    print(type(segments))\n",
    "    print(segments[0])\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, lengths, thetas = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.cos(thetas)\n",
    "    y1s = y0s + lengths * torch.sin(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt((nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "segments = data\n",
    "centroid = torch.tensor([2.0, 2.0])  # [x0, y0]\n",
    "\n",
    "distances = compute_distances(segments, centroid)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00930561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_distances_2d(segments, centroid):\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, lengths, thetas = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.cos(thetas)\n",
    "    y1s = y0s + lengths * torch.sin(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt((nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def compute_distances_3d(segments, centroid):\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy, cz = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, z0s, lengths, thetas, phis = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "        segments[:, 4],\n",
    "        segments[:, 5],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.sin(thetas) * torch.cos(phis)\n",
    "    y1s = y0s + lengths * torch.sin(thetas) * torch.sin(phis)\n",
    "    z1s = z0s + lengths * torch.cos(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s, cz - z0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s, z1s - z0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    nearest_zs = z0s + projection_scalars * (z1s - z0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt(\n",
    "        (nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2 + (nearest_zs - cz) ** 2\n",
    "    )\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def get_dist_matrix(data, centroids, dist_function):\n",
    "    # init.\n",
    "    dist_matrix = torch.zeros(data.shape[0], centroids.shape[0])\n",
    "    for i in range(centroids.shape[0]):\n",
    "        dist_matrix[:, i] = dist_function(data, centroids[i])\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def kmeans_pp_greedy(\n",
    "    data,\n",
    "    n_clusters,\n",
    "    dist_function,\n",
    "    dim,\n",
    "    random_state=None,\n",
    "    n_trials=None,\n",
    "):\n",
    "    # check parameters\n",
    "    if type(data) is not torch.Tensor:\n",
    "        data = torch.tensor(data)\n",
    "    n_samples, n_features = data.shape\n",
    "    if n_clusters > n_samples:\n",
    "        raise ValueError(\n",
    "            \"n_clusters should be smaller or equal to the number of centroids\"\n",
    "        )\n",
    "    if type(n_clusters) is not torch.Tensor:\n",
    "        n_clusters = torch.tensor(n_clusters)\n",
    "    if n_trials is None:\n",
    "        n_trials = 2 + int(torch.log(n_clusters))\n",
    "    # set random state\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "    # initialize centroids\n",
    "    centroids = torch.zeros(n_clusters, dim)\n",
    "    # choose first centroid\n",
    "    first_centroid_idx = torch.randint(n_samples, (1,))\n",
    "    # print(f\"First centroid index: {first_centroid_idx}\")\n",
    "    data_oi = data[first_centroid_idx]\n",
    "    if dim == 2:\n",
    "        x0, y0, l, theta = data_oi[0]\n",
    "        centroids[0] = torch.tensor([x0, y0])\n",
    "        centroids[0] += torch.tensor([0.5 * l * torch.cos(theta), l * torch.sin(theta)])\n",
    "    elif dim == 3:\n",
    "        x0, y0, z0, l, theta, phi = data_oi[0]\n",
    "        centroids[0] = torch.tensor([x0, y0, z0])\n",
    "        centroids[0] += torch.tensor(\n",
    "            [\n",
    "                0.5 * l * torch.sin(theta) * torch.cos(phi),\n",
    "                0.5 * l * torch.sin(theta) * torch.sin(phi),\n",
    "                0.5 * l * torch.cos(theta),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"dim should be 2 or 3\")\n",
    "\n",
    "    # create a vector of minus ones of shape (n_samples,)\n",
    "    indices = -torch.ones(n_samples)\n",
    "    # init dist matrix\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Centroids shape: {centroids.shape}\")\n",
    "    print(f\"Dist function: {dist_function}\")\n",
    "    dist_matrix = get_dist_matrix(data, centroids[:1], dist_function)\n",
    "\n",
    "    for i in range(1, n_clusters):\n",
    "        # print(\"=\" * 20)\n",
    "        # print(\"=\" * 20)\n",
    "        # choose the next centroid\n",
    "        freq = torch.zeros(n_trials)\n",
    "        for _ in range(n_trials):\n",
    "            # choose a centroid with probability proportional to the distance\n",
    "            # to the closest centroid\n",
    "            dists = torch.min(dist_matrix, dim=1).values\n",
    "            probs = dists / torch.sum(dists)\n",
    "            # print(f\"Probs : {probs}\")\n",
    "            next_centroid_idx = torch.multinomial(probs, 1)\n",
    "            # print(f\"Next centroid index: {next_centroid_idx}\")\n",
    "            # update best_dist\n",
    "            freq[_] = next_centroid_idx\n",
    "        # pick randomly a vlaue from freq\n",
    "        idx = torch.randint(n_trials, (1,))\n",
    "        next_centroid_idx = freq[idx].int()\n",
    "        # print(\"=\" * 20)\n",
    "        # print(f\"Next centroid index: {next_centroid_idx}\")\n",
    "        # print(f\"Next data: {data[next_centroid_idx]}\")\n",
    "        # print(\"=\" * 20)\n",
    "        # choose the centroid with the smallest best_dist\n",
    "        data_oi = data[next_centroid_idx]\n",
    "        if dim == 2:\n",
    "            x0, y0, l, theta = data_oi[0]\n",
    "            centroids[i] = torch.tensor([x0, y0])\n",
    "            centroids[i] += torch.tensor(\n",
    "                [0.5 * l * torch.cos(theta), l * torch.sin(theta)]\n",
    "            )\n",
    "        elif dim == 3:\n",
    "            x0, y0, z0, l, theta, phi = data_oi[0]\n",
    "            centroids[i] = torch.tensor([x0, y0, z0])\n",
    "            centroids[i] += torch.tensor(\n",
    "                [\n",
    "                    0.5 * l * torch.sin(theta) * torch.cos(phi),\n",
    "                    0.5 * l * torch.sin(theta) * torch.sin(phi),\n",
    "                    0.5 * l * torch.cos(theta),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"dim should be 2 or 3\")\n",
    "\n",
    "        # update dist_matrix\n",
    "        dist_matrix = get_dist_matrix(data, centroids[: i + 1], dist_function)\n",
    "        # print(dist_matrix.shape)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_2d(segments, centroid):\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, lengths, thetas = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.cos(thetas)\n",
    "    y1s = y0s + lengths * torch.sin(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt((nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "centroid = torch.tensor([393, 228])  # [x0, y0]\n",
    "# lets plot them\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segments = data2d[0]\n",
    "plt.plot(\n",
    "    [segments[0], segments[0] + segments[2] * torch.cos(segments[3])],\n",
    "    [segments[1], segments[1] + segments[2] * torch.sin(segments[3])],\n",
    ")\n",
    "plt.scatter(centroid[0], centroid[1], color=\"r\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "# find their distance\n",
    "segments = data2d[:1]\n",
    "print(segments)\n",
    "distances = compute_distances_2d(segments, centroid)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f17aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLS:\n",
    "    def __init__(self, data, k, dim, dist_function):\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.dist_function = dist_function\n",
    "\n",
    "    def get_points_from_emb(self, data, dim):\n",
    "        \"\"\"\n",
    "        Get the points from the embedding\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch.Tensor\n",
    "            The data tensor\n",
    "        dim : int\n",
    "            The dimension of the space\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The points in the space [x0,y0,x1,y1,...]\n",
    "        \"\"\"\n",
    "        data_points = torch.zeros(data.shape[0], 2 * dim)\n",
    "        if dim == 2:  # [x0,y0,l,theta]\n",
    "            data_points[:, 0] = data[:, 0]\n",
    "            data_points[:, 1] = data[:, 1]\n",
    "            data_points[:, 2] = data[:, 0] + data[:, 2] * torch.cos(data[:, 3])\n",
    "            data_points[:, 3] = data[:, 1] + data[:, 2] * torch.sin(data[:, 3])\n",
    "        elif dim == 3:  # [x0,y0,z0,l,theta,phi]\n",
    "            data_points[:, 0] = data[:, 0]\n",
    "            data_points[:, 1] = data[:, 1]\n",
    "            data_points[:, 2] = data[:, 2]\n",
    "            data_points[:, 3] = data[:, 0] + data[:, 3] * torch.sin(\n",
    "                data[:, 4]\n",
    "            ) * torch.cos(data[:, 5])\n",
    "            data_points[:, 4] = data[:, 1] + data[:, 3] * torch.sin(\n",
    "                data[:, 4]\n",
    "            ) * torch.sin(data[:, 5])\n",
    "            data_points[:, 5] = data[:, 2] + data[:, 3] * torch.cos(data[:, 4])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"dim should be 2 or 3\")\n",
    "        return data_points\n",
    "\n",
    "    def fit(self, n_iter, n_trials=10):\n",
    "        self.data_points = self.get_points_from_emb(self.data, self.dim)\n",
    "        # initialize using kmeans++ greedy\n",
    "        centroids = kmeans_pp_greedy(\n",
    "            self.data, self.k, self.dist_function, self.dim, n_trials=n_trials\n",
    "        )\n",
    "        # get initial divergence\n",
    "        dist_matrix = get_dist_matrix(self.data, centroids, self.dist_function)\n",
    "        div = torch.sum(torch.min(dist_matrix, dim=1).values)\n",
    "        print(f\"Initial divergence: {div}\")\n",
    "\n",
    "        # do the iterations\n",
    "        for i in range(n_iter):\n",
    "            # get the distances\n",
    "            dist_matrix = get_dist_matrix(self.data, centroids, self.dist_function)\n",
    "            # get the labels\n",
    "            labels = torch.argmin(dist_matrix, dim=1)\n",
    "            # update the centroids\n",
    "            for j in range(self.k):\n",
    "                dp = self.data_points[labels == j]\n",
    "                centroids[j][0] = torch.mean(torch.concatenate([dp[:, 0], dp[:, 2]]))\n",
    "                centroids[j][1] = torch.mean(torch.concatenate([dp[:, 1], dp[:, 3]]))\n",
    "                if self.dim == 3:\n",
    "                    centroids[j][2] = torch.mean(\n",
    "                        torch.concatenate([dp[:, 2], dp[:, 4]])\n",
    "                    )\n",
    "            # get the divergence\n",
    "            div = torch.sum(torch.min(dist_matrix, dim=1).values)\n",
    "            print(f\"Iteration {i+1}, divergence: {div}\")\n",
    "        self.centroids = centroids\n",
    "\n",
    "    def predict(self, centroids):\n",
    "        # get the distances\n",
    "        dist_matrix = get_dist_matrix(self.data, centroids, self.dist_function)\n",
    "        # get the labels\n",
    "        labels = torch.argmin(dist_matrix, dim=1)\n",
    "        return labels\n",
    "\n",
    "\n",
    "# kmeans = ClusteringLS(data, 3, 2, compute_distances_2d)\n",
    "# kmeans.fit(10)\n",
    "# centroids = kmeans.centroids\n",
    "# plt.plot([data_points[:, 0], data_points[:, 2]], [data_points[:, 1], data_points[:, 3]])\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1])\n",
    "# plt.axis(\"equal\")\n",
    "# plt.show()\n",
    "kmeans = ClusteringLS(data3d, 3, 3, compute_distances_3d)\n",
    "kmeans.fit(10)\n",
    "centroids = kmeans.centroids\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "for i in range(kmeans.data_points.shape[0]):\n",
    "    ax.plot(\n",
    "        [kmeans.data_points[i, 0], kmeans.data_points[i, 3]],\n",
    "        [kmeans.data_points[i, 1], kmeans.data_points[i, 4]],\n",
    "        [kmeans.data_points[i, 2], kmeans.data_points[i, 5]],\n",
    "    )\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c=\"r\", s=100)\n",
    "ax.set_xlim([bbox[0], bbox[1]])\n",
    "ax.set_ylim([bbox[2], bbox[3]])\n",
    "ax.set_zlim([bbox[4], bbox[5]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5763086fb5664",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create the k-tree\n",
    "\n",
    "### Train the k-tree\n",
    "\n",
    "Starting from the tree's root node, train the *Clustering* and *Critic* models, store the Critic model on the node and assign the data on its children according to the Critic's predictions.\n",
    "Repeat recursively the process until a node has less amount of data than the threshold.\n",
    "\n",
    "Also plot some training results and store them as well as the resulting models to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53242dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 100 line segments\n",
      "x0 shape: torch.Size([100])\n",
      "l shape: torch.Size([100])\n",
      "theta shape: torch.Size([100])\n",
      "Data shape: torch.Size([100, 4])\n",
      "Data points shape: torch.Size([100, 4])\n",
      "Starting to create the tree...\n",
      "====================\n",
      "\n",
      "====================\n",
      "Creating critic for node 0 that has 100 data, which is more than the threshold 20.\n",
      "Bounding box for node 0: tensor([[ -54.2657, 1081.0024],\n",
      "        [   6.6679,  986.5159]], device='cuda:0')\n",
      "Creating clustering for node 0 with 3 centroids.\n",
      "Initial divergence: 27895.037109375\n",
      "Iteration 1, divergence: 27895.037109375\n",
      "Iteration 2, divergence: 21770.939453125\n",
      "Iteration 3, divergence: 20838.443359375\n",
      "Iteration 4, divergence: 20745.76171875\n",
      "Iteration 5, divergence: 20799.18359375\n",
      "Iteration 6, divergence: 20799.18359375\n",
      "Iteration 7, divergence: 20799.18359375\n",
      "Iteration 8, divergence: 20799.18359375\n",
      "Iteration 9, divergence: 20799.18359375\n",
      "Iteration 10, divergence: 20799.18359375\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[239.53639 348.61758]\n",
      " [595.9562  796.7939 ]\n",
      " [779.154   321.14343]]\n",
      "scale is 1135.26806640625\n",
      "Processing...\n",
      "flag is 1617\n",
      "m is 884\n",
      "i is 2500\n",
      "Labeled 0/884 points.\n",
      "Labeled all 884/884 points.\n",
      "Creating critic for node 0 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.31447963800904977\n",
      "Epoch:  0 Cost:  1889.441162109375\n",
      "Acc:  0.7239819004524887\n",
      "Epoch:  200 Cost:  53.72736358642578\n",
      "Acc:  0.8540723981900452\n",
      "Epoch:  400 Cost:  41.167240142822266\n",
      "Acc:  0.8970588235294118\n",
      "Epoch:  600 Cost:  34.078372955322266\n",
      "Acc:  0.9095022624434389\n",
      "Epoch:  800 Cost:  31.00302505493164\n",
      "Acc:  0.9366515837104072\n",
      "Epoch:  1000 Cost:  22.78818702697754\n",
      "Acc:  0.8947963800904978\n",
      "Epoch:  1200 Cost:  26.237154006958008\n",
      "Acc:  0.7319004524886877\n",
      "Epoch:  1400 Cost:  51.8574104309082\n",
      "Acc:  0.8891402714932126\n",
      "Epoch:  1600 Cost:  34.51413345336914\n",
      "Acc:  0.9140271493212669\n",
      "Epoch:  1800 Cost:  22.68047523498535\n",
      "Saved critic config to ./models/line_segments/100/0_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/0_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 00 that has 31 data, which is more than the threshold 20.\n",
      "Bounding box for node 00: tensor([[-54.2657, 555.5686],\n",
      "        [  6.6679, 811.3831]], device='cuda:0')\n",
      "Creating clustering for node 00 with 3 centroids.\n",
      "Initial divergence: 3361.611083984375\n",
      "Iteration 1, divergence: 3361.611083984375\n",
      "Iteration 2, divergence: 3302.38330078125\n",
      "Iteration 3, divergence: 3302.38330078125\n",
      "Iteration 4, divergence: 3302.38330078125\n",
      "Iteration 5, divergence: 3302.38330078125\n",
      "Iteration 6, divergence: 3302.38330078125\n",
      "Iteration 7, divergence: 3302.38330078125\n",
      "Iteration 8, divergence: 3302.38330078125\n",
      "Iteration 9, divergence: 3302.38330078125\n",
      "Iteration 10, divergence: 3302.38330078125\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[223.60362 194.61693]\n",
      " [406.40988 455.94977]\n",
      " [ 92.64896 685.39874]]\n",
      "scale is 804.7152099609375\n",
      "Processing...\n",
      "flag is 1562\n",
      "m is 939\n",
      "i is 2500\n",
      "Labeled 0/939 points.\n",
      "Labeled all 939/939 points.\n",
      "Creating critic for node 00 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.4749733759318424\n",
      "Epoch:  0 Cost:  822.6088256835938\n",
      "Acc:  0.7135250266240681\n",
      "Epoch:  200 Cost:  55.16794204711914\n",
      "Acc:  0.8029818956336529\n",
      "Epoch:  400 Cost:  42.559898376464844\n",
      "Acc:  0.9084132055378061\n",
      "Epoch:  600 Cost:  29.999759674072266\n",
      "Acc:  0.8146964856230032\n",
      "Epoch:  800 Cost:  49.00114059448242\n",
      "Acc:  0.9627263045793397\n",
      "Epoch:  1000 Cost:  17.761035919189453\n",
      "Acc:  0.9456869009584664\n",
      "Epoch:  1200 Cost:  15.323038101196289\n",
      "Acc:  0.9510117145899893\n",
      "Epoch:  1400 Cost:  12.771007537841797\n",
      "Acc:  0.7838125665601704\n",
      "Epoch:  1600 Cost:  69.24281311035156\n",
      "Acc:  0.8221512247071352\n",
      "Epoch:  1800 Cost:  42.54127883911133\n",
      "Saved critic config to ./models/line_segments/100/00_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/00_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 01 that has 34 data, which is more than the threshold 20.\n",
      "Bounding box for node 01: tensor([[  44.0955, 1033.0614],\n",
      "        [ 554.2108,  986.5159]], device='cuda:0')\n",
      "Creating clustering for node 01 with 3 centroids.\n",
      "Initial divergence: 7904.6572265625\n",
      "Iteration 1, divergence: 7904.6572265625\n",
      "Iteration 2, divergence: 4990.85302734375\n",
      "Iteration 3, divergence: 4208.99658203125\n",
      "Iteration 4, divergence: 4163.26171875\n",
      "Iteration 5, divergence: 4105.837890625\n",
      "Iteration 6, divergence: 4105.837890625\n",
      "Iteration 7, divergence: 4105.837890625\n",
      "Iteration 8, divergence: 4105.837890625\n",
      "Iteration 9, divergence: 4105.837890625\n",
      "Iteration 10, divergence: 4105.837890625\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[756.2203   754.0817  ]\n",
      " [400.03955  847.1334  ]\n",
      " [127.162834 927.98065 ]]\n",
      "scale is 988.9658813476562\n",
      "Processing...\n",
      "flag is 1283\n",
      "m is 1218\n",
      "i is 2500\n",
      "Labeled 0/1218 points.\n",
      "Labeled 1000/1218 points.\n",
      "Labeled all 1218/1218 points.\n",
      "Creating critic for node 01 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.29802955665024633\n",
      "Epoch:  0 Cost:  679.34619140625\n",
      "Acc:  0.8407224958949097\n",
      "Epoch:  200 Cost:  29.892269134521484\n",
      "Acc:  0.8505747126436781\n",
      "Epoch:  400 Cost:  27.658239364624023\n",
      "Acc:  0.8587848932676518\n",
      "Epoch:  600 Cost:  25.52167510986328\n",
      "Acc:  0.896551724137931\n",
      "Epoch:  800 Cost:  28.097566604614258\n",
      "Acc:  0.8694581280788177\n",
      "Epoch:  1000 Cost:  24.354280471801758\n",
      "Acc:  0.9055829228243021\n",
      "Epoch:  1200 Cost:  21.056747436523438\n",
      "Acc:  0.9195402298850575\n",
      "Epoch:  1400 Cost:  19.788799285888672\n",
      "Acc:  0.9302134646962233\n",
      "Epoch:  1600 Cost:  17.06793975830078\n",
      "Acc:  0.9277504105090312\n",
      "Epoch:  1800 Cost:  17.385934829711914\n",
      "Saved critic config to ./models/line_segments/100/01_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/01_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 02 that has 35 data, which is more than the threshold 20.\n",
      "Bounding box for node 02: tensor([[ 450.9106, 1081.0024],\n",
      "        [  29.4423,  592.3256]], device='cuda:0')\n",
      "Creating clustering for node 02 with 3 centroids.\n",
      "Initial divergence: 5055.36328125\n",
      "Iteration 1, divergence: 5055.36328125\n",
      "Iteration 2, divergence: 3842.433349609375\n",
      "Iteration 3, divergence: 3505.785400390625\n",
      "Iteration 4, divergence: 3483.902099609375\n",
      "Iteration 5, divergence: 3483.902099609375\n",
      "Iteration 6, divergence: 3483.902099609375\n",
      "Iteration 7, divergence: 3483.902099609375\n",
      "Iteration 8, divergence: 3483.902099609375\n",
      "Iteration 9, divergence: 3483.902099609375\n",
      "Iteration 10, divergence: 3483.902099609375\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[819.10455 186.25595]\n",
      " [873.11273 504.50693]\n",
      " [619.8685  308.28622]]\n",
      "scale is 630.091796875\n",
      "Processing...\n",
      "flag is 1586\n",
      "m is 915\n",
      "i is 2500\n",
      "Labeled 0/915 points.\n",
      "Labeled all 915/915 points.\n",
      "Creating critic for node 02 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.28743169398907104\n",
      "Epoch:  0 Cost:  800.1632690429688\n",
      "Acc:  0.7049180327868853\n",
      "Epoch:  200 Cost:  57.59162902832031\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  400 Cost:  106.0662612915039\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  600 Cost:  106.06613159179688\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  800 Cost:  106.06611633300781\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  1000 Cost:  106.06611633300781\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  1200 Cost:  106.06611633300781\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  1400 Cost:  106.06611633300781\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  1600 Cost:  106.06611633300781\n",
      "Acc:  0.41857923497267757\n",
      "Epoch:  1800 Cost:  106.06611633300781\n",
      "Saved critic config to ./models/line_segments/100/02_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/02_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 010 that has 21 data, which is more than the threshold 20.\n",
      "Bounding box for node 010: tensor([[ 517.3738, 1033.0614],\n",
      "        [ 554.2108,  943.9302]], device='cuda:0')\n",
      "Creating clustering for node 010 with 3 centroids.\n",
      "Initial divergence: 2151.48828125\n",
      "Iteration 1, divergence: 2151.48828125\n",
      "Iteration 2, divergence: 1807.4742431640625\n",
      "Iteration 3, divergence: 1466.07861328125\n",
      "Iteration 4, divergence: 1391.9033203125\n",
      "Iteration 5, divergence: 1391.9033203125\n",
      "Iteration 6, divergence: 1391.9033203125\n",
      "Iteration 7, divergence: 1391.9033203125\n",
      "Iteration 8, divergence: 1391.9033203125\n",
      "Iteration 9, divergence: 1391.9033203125\n",
      "Iteration 10, divergence: 1391.9033203125\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[912.06506 798.07574]\n",
      " [679.69684 626.7637 ]\n",
      " [677.24866 811.07556]]\n",
      "scale is 515.6876220703125\n",
      "Processing...\n",
      "flag is 1563\n",
      "m is 938\n",
      "i is 2500\n",
      "Labeled 0/938 points.\n",
      "Labeled all 938/938 points.\n",
      "Creating critic for node 010 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.09808102345415778\n",
      "Epoch:  0 Cost:  1205.9423828125\n",
      "Acc:  0.7100213219616205\n",
      "Epoch:  200 Cost:  55.04440689086914\n",
      "Acc:  0.7953091684434968\n",
      "Epoch:  400 Cost:  51.81261444091797\n",
      "Acc:  0.7931769722814499\n",
      "Epoch:  600 Cost:  48.99137496948242\n",
      "Acc:  0.8294243070362474\n",
      "Epoch:  800 Cost:  40.453277587890625\n",
      "Acc:  0.8752665245202559\n",
      "Epoch:  1000 Cost:  34.674739837646484\n",
      "Acc:  0.8336886993603412\n",
      "Epoch:  1200 Cost:  39.17300796508789\n",
      "Acc:  0.5991471215351812\n",
      "Epoch:  1400 Cost:  120.30413055419922\n",
      "Acc:  0.746268656716418\n",
      "Epoch:  1600 Cost:  64.06124877929688\n",
      "Acc:  0.7078891257995735\n",
      "Epoch:  1800 Cost:  59.87607192993164\n",
      "Saved critic config to ./models/line_segments/100/010_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/010_critic_training_results.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.k_tree import Ktree\n",
    "from src.k_tree_ls import Ktree as Ktree_ls\n",
    "from src.utils.objects.squares import loadData as loadSquares\n",
    "from src.metrics import Linf_simple\n",
    "from src.utils import plot_tools as pt\n",
    "from src.utils import accuracy as acc\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "# lets generate #n line segments inside a bbox\n",
    "n = 100\n",
    "bbox = [0, 1000, 0, 1000]  # bbox\n",
    "length_size = [0.1, 100]  # length of the line segment\n",
    "theta_size = [0, 180]  # angle of the line segment\n",
    "\n",
    "\n",
    "# line segments will be [x0,y0,l,theta]\n",
    "# sample x0,y0 inside bbox\n",
    "def create_line_segments(n, bbox, length_size, theta_size):\n",
    "    print(f\"Creating {n} line segments\")\n",
    "    x0 = torch.rand(n) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "    y0 = torch.rand(n) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "    print(f\"x0 shape: {x0.shape}\")\n",
    "    # sample l inside length_size\n",
    "    l = torch.rand(n) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "    print(f\"l shape: {l.shape}\")\n",
    "    # sample theta inside theta_size\n",
    "    theta_size = torch.rand(n) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "    print(f\"theta shape: {theta_size.shape}\")\n",
    "    theta_size = torch.deg2rad(theta_size)\n",
    "    return torch.stack([x0, y0, l, theta_size], dim=1)\n",
    "\n",
    "\n",
    "data = create_line_segments(n, bbox, length_size, theta_size)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "# first make data from embedding to datapoints\n",
    "data_points = torch.zeros(n, 4)\n",
    "data_points[:, 0] = data[:, 0]\n",
    "data_points[:, 1] = data[:, 1]\n",
    "data_points[:, 2] = data[:, 0] + data[:, 2] * torch.cos(data[:, 3])\n",
    "data_points[:, 3] = data[:, 1] + data[:, 2] * torch.sin(data[:, 3])\n",
    "print(f\"Data points shape: {data_points.shape}\")\n",
    "data2d = data\n",
    "\n",
    "dim = 2  # space dimension\n",
    "\n",
    "k = 3  # number of centroids to generate in the Clustering model\n",
    "clustering_args = {\n",
    "    \"epochs\": 10,  # number of epochs\n",
    "    \"pre_processing\": 10,\n",
    "    \"number_of_centroids\": k,  # number of centroids to generate in the Clustering model\n",
    "    \"dimension\": dim,  # space dimension\n",
    "}\n",
    "n = 50\n",
    "un_args = {\n",
    "    \"N\": n,  # number of points to sample\n",
    "    \"M\": n**2 - 1,  # number of points to return\n",
    "    \"epsilon\": 0.15,  # the epsilon ball\n",
    "}\n",
    "critic_args = {\n",
    "    \"optimizer_lr\": 5e-3,  # optimiser learning rate\n",
    "    \"epochs\": 2000,  # number of epochs\n",
    "    \"width\": 300,  # width of the model's linear layers\n",
    "    \"depth\": 5,  # depth of the model's linear layers\n",
    "}\n",
    "\n",
    "threshold = 20  # if a tree node has data less than the threshold, stop division\n",
    "\n",
    "# Initialise the k-tree structure.\n",
    "from src.metrics import compute_distances_2d\n",
    "\n",
    "# pass data to the k-tree\n",
    "distance_function = compute_distances_2d\n",
    "ktree = Ktree_ls(\n",
    "    threshold,\n",
    "    data2d,\n",
    "    distance_function,\n",
    "    clustering_args,\n",
    "    un_args,\n",
    "    critic_args,\n",
    "    device,\n",
    "    dim,\n",
    ")\n",
    "# pass data to device\n",
    "data = data.to(device)\n",
    "print(\"Starting to create the tree...\")\n",
    "print(\"=\" * 20)\n",
    "ktree.create_tree(save_path_prefix=\"./models/line_segments/100/\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f686891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to create the tree...\n",
      "====================\n",
      "\n",
      "====================\n",
      "Creating critic for node 0 that has 100 data, which is more than the threshold 70.\n",
      "Bounding box for node 0: tensor([[-80.6067, 578.3220],\n",
      "        [  8.3378, 499.1962],\n",
      "        [  2.1722, 502.9510]])\n",
      "Creating clustering for node 0 with 3 centroids.\n",
      "Initial divergence: 19544.208984375\n",
      "Iteration 1, divergence: 19544.208984375\n",
      "Iteration 2, divergence: 17797.6171875\n",
      "Iteration 3, divergence: 17946.44140625\n",
      "Iteration 4, divergence: 18211.447265625\n",
      "Iteration 5, divergence: 18249.380859375\n",
      "Iteration 6, divergence: 17991.3125\n",
      "Iteration 7, divergence: 18008.6640625\n",
      "Iteration 8, divergence: 17896.3671875\n",
      "Iteration 9, divergence: 17970.51953125\n",
      "Iteration 10, divergence: 18139.080078125\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[254.91885 243.3715  181.19452]\n",
      " [361.80273 350.58932 356.33035]\n",
      " [193.85045 260.20886 295.26947]]\n",
      "scale is 658.9287109375\n",
      "dx is 658.9287109375, dy is 490.8583984375, dz is 500.77886962890625\n",
      "z is torch.Size([27000])\n",
      "Processing...\n",
      "flag is 5191\n",
      "m is 21810\n",
      "i is 27000\n",
      "Labeled 0/21810 points.\n",
      "Labeled 1000/21810 points.\n",
      "Labeled 2000/21810 points.\n",
      "Labeled 3000/21810 points.\n",
      "Labeled 4000/21810 points.\n",
      "Labeled 5000/21810 points.\n",
      "Labeled 6000/21810 points.\n",
      "Labeled 7000/21810 points.\n",
      "Labeled 8000/21810 points.\n",
      "Labeled 9000/21810 points.\n",
      "Labeled 10000/21810 points.\n",
      "Labeled 11000/21810 points.\n",
      "Labeled 12000/21810 points.\n",
      "Labeled 13000/21810 points.\n",
      "Labeled 14000/21810 points.\n",
      "Labeled 15000/21810 points.\n",
      "Labeled 16000/21810 points.\n",
      "Labeled 17000/21810 points.\n",
      "Labeled 18000/21810 points.\n",
      "Labeled 19000/21810 points.\n",
      "Labeled 20000/21810 points.\n",
      "Labeled 21000/21810 points.\n",
      "Labeled all 21810/21810 points.\n",
      "Creating critic for node 0 with 3 centroids.\n",
      "Device is: cpu\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.2500229252636405\n",
      "Epoch:  0 Cost:  256.1949462890625\n",
      "Acc:  0.8784502521779001\n",
      "Epoch:  200 Cost:  27.53789520263672\n",
      "Acc:  0.9146263182026594\n",
      "Epoch:  400 Cost:  17.48084831237793\n",
      "Acc:  0.9110958276020175\n",
      "Epoch:  600 Cost:  17.58210563659668\n",
      "Acc:  0.9312242090784044\n",
      "Epoch:  800 Cost:  14.209858894348145\n",
      "Acc:  0.9362219165520403\n",
      "Epoch:  1000 Cost:  12.9972562789917\n",
      "Acc:  0.9533700137551582\n",
      "Epoch:  1200 Cost:  10.13469409942627\n",
      "Acc:  0.974828060522696\n",
      "Epoch:  1400 Cost:  7.028074741363525\n",
      "Acc:  0.9540577716643741\n",
      "Epoch:  1600 Cost:  9.713964462280273\n",
      "Acc:  0.9696928014672169\n",
      "Epoch:  1800 Cost:  6.983205795288086\n",
      "Saved critic config to ./models/line_segments/3d/100/0_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/3d/100/0_critic_training_results.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.k_tree import Ktree\n",
    "from src.k_tree_ls import Ktree as Ktree_ls\n",
    "from src.utils.objects.squares import loadData as loadSquares\n",
    "from src.metrics import Linf_simple\n",
    "from src.utils import plot_tools as pt\n",
    "from src.utils import accuracy as acc\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# create 3D line_segments\n",
    "# n = 10000\n",
    "# lim = 50000\n",
    "# var = 500\n",
    "n = 100\n",
    "lim = 500\n",
    "var = 100\n",
    "\n",
    "bbox = [0, lim, 0, lim, 0, lim]\n",
    "length_size = [0.1, var]\n",
    "theta_size = [0, 180]\n",
    "phi_size = [0, 360]\n",
    "\n",
    "\n",
    "def create_line_segments_3d(n, bbox, length_size, theta_size, phi_size):\n",
    "    line_segments = torch.zeros((n, 6))  # ls = [x0,y0,z0, l, theta, phi]\n",
    "    for i in range(n):\n",
    "        x0 = torch.rand(1) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "        y0 = torch.rand(1) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "        z0 = torch.rand(1) * (bbox[5] - bbox[4]) + bbox[4]\n",
    "        l = torch.rand(1) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "        theta = torch.rand(1) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "        theta = torch.deg2rad(theta)\n",
    "        phi = torch.rand(1) * (phi_size[1] - phi_size[0]) + phi_size[0]\n",
    "        phi = torch.deg2rad(phi)\n",
    "        line_segments[i, :] = torch.cat([x0, y0, z0, l, theta, phi])\n",
    "    return line_segments\n",
    "\n",
    "\n",
    "data = create_line_segments_3d(n, bbox, length_size, theta_size, phi_size)\n",
    "# make the points\n",
    "data_points = torch.zeros((n, 6))\n",
    "for i in range(n):\n",
    "    data_points[i, 0] = data[i, 0]\n",
    "    data_points[i, 1] = data[i, 1]\n",
    "    data_points[i, 2] = data[i, 2]\n",
    "    data_points[i, 3] = data[i, 0] + data[i, 3] * torch.sin(data[i, 4]) * torch.cos(\n",
    "        data[i, 5]\n",
    "    )\n",
    "    data_points[i, 4] = data[i, 1] + data[i, 3] * torch.sin(data[i, 4]) * torch.sin(\n",
    "        data[i, 5]\n",
    "    )\n",
    "    data_points[i, 5] = data[i, 2] + data[i, 3] * torch.cos(data[i, 4])\n",
    "data3d = data\n",
    "\n",
    "\n",
    "dim = 3  # space dimension\n",
    "\n",
    "k = 3  # number of centroids to generate in the Clustering model\n",
    "clustering_args = {\n",
    "    \"epochs\": 10,  # number of epochs\n",
    "    \"pre_processing\": 10,\n",
    "    \"number_of_centroids\": k,  # number of centroids to generate in the Clustering model\n",
    "    \"dimension\": dim,  # space dimension\n",
    "}\n",
    "n = 30\n",
    "un_args = {\n",
    "    \"N\": n,  # number of points to sample\n",
    "    \"M\": n**dim - 1,  # number of points to return\n",
    "    \"epsilon\": 0.15,  # the epsilon ball\n",
    "}\n",
    "critic_args = {\n",
    "    \"optimizer_lr\": 5e-3,  # optimiser learning rate\n",
    "    \"epochs\": 2000,  # number of epochs\n",
    "    \"width\": 300,  # width of the model's linear layers\n",
    "    \"depth\": 5,  # depth of the model's linear layers\n",
    "}\n",
    "\n",
    "threshold = 70  # if a tree node has data less than the threshold, stop division\n",
    "\n",
    "# Initialise the k-tree structure.\n",
    "from src.metrics import compute_distances_3d\n",
    "\n",
    "# pass data to the k-tree\n",
    "distance_function = compute_distances_3d\n",
    "ktree = Ktree_ls(\n",
    "    threshold,\n",
    "    data,\n",
    "    distance_function,\n",
    "    clustering_args,\n",
    "    un_args,\n",
    "    critic_args,\n",
    "    device,\n",
    "    dim,\n",
    ")\n",
    "# pass data to device\n",
    "data = data.to(device)\n",
    "print(\"Starting to create the tree...\")\n",
    "print(\"=\" * 20)\n",
    "ktree.create_tree(save_path_prefix=\"./models/line_segments/3d/100/\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eca03c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 97}\n",
      "{'0': 97, '00': 86}\n",
      "{'0': 97, '00': 86, '01': 87}\n",
      "{'0': 97, '00': 86, '01': 87, '02': 80}\n",
      "{'0': 97, '00': 86, '01': 87, '02': 80, '010': 79}\n",
      "{'0': 0.97, '00': 0.86, '01': 0.87, '02': 0.8, '010': 0.79}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# np.random.seed(0)\n",
    "n = 100\n",
    "random_points = torch.zeros(n, ktree.dim)\n",
    "space = ktree.root.get_bounding_box().to(\"cpu\")  # get the bounding box of the tree\n",
    "for i in range(n):\n",
    "    random_points[i] = torch.tensor(\n",
    "        # [np.random.uniform(space[d][0], space[d][1]) for d in range(ktree.dim)]\n",
    "        [\n",
    "            torch.rand(1) * (space[d][1] - space[d][0]) + space[d][0]\n",
    "            for d in range(ktree.dim)\n",
    "        ]\n",
    "    )\n",
    "# pass points on device\n",
    "# make them torch\n",
    "random_points = random_points.to(device)\n",
    "st_acc = ktree.get_critic_accuracies(random_points)\n",
    "\n",
    "print(st_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d8d22e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of queries per layer are:\n",
      "[9. 9. 1.]\n",
      "The percentage of correct predictions per layer is: \n",
      "[90. 90. 90.]\n"
     ]
    }
   ],
   "source": [
    "acc.serialised_queries(ktree, n=10, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14c98993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912673"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.97**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ca19b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of queries per layer are:\n",
      "[100.  99.  17.]\n",
      "The percentage of correct predictions per layer is:\n",
      "[99. 95. 91.]\n",
      "The number of queries per layer are:\n",
      "[100.  99.  15.]\n",
      "The percentage of correct predictions per layer is:\n",
      "[99. 94. 93.]\n",
      "The number of queries per layer are:\n",
      "[100.  99.  18.]\n",
      "The percentage of correct predictions per layer is:\n",
      "[99. 94. 89.]\n",
      "The mean percentage of correct predictions per layer is:\n",
      "[99.         94.33333333 91.        ]\n"
     ]
    }
   ],
   "source": [
    "acc.random_queries(ktree, n=100, times=3, k=2)\n",
    "# acc.serialised_queries(ktree, n=10, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e032a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912673"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.97**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c300356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9695)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.tensor(0.94))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc44aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree node number is 16\n",
      "Tree height is 4.\n",
      "Created 11 leaves with sizes\n",
      "[18, 7, 6, 7, 6, 8, 10, 3, 14, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tree node number is {ktree.number_of_nodes}\")\n",
    "\n",
    "leaves = ktree.get_leaves()\n",
    "\n",
    "height = max([len(leaf.index) for leaf in leaves])\n",
    "print(f\"Tree height is {height}.\")\n",
    "\n",
    "leaf_sizes = [len(leaf.data) for leaf in leaves]\n",
    "print(f\"Created {len(leaves)} leaves with sizes\")\n",
    "print(leaf_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c94cd5651af86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:14:48.624832180Z",
     "start_time": "2024-01-10T12:05:09.378879820Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ktree.create_tree(save_path_prefix=\"./models/squares/1000/demo\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60626e8af3d794",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the k-tree from saved configuration\n",
    "\n",
    "For a k-tree instance that is already trained and saved to configuration files (all existing in the same path and having the same prefix), load the instance by simply specifying that path and prefix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a04a448f5e397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:14:51.507813809Z",
     "start_time": "2024-01-10T12:14:48.677600920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Critic's architecture arguments are still required.\n",
    "critic_args = {\n",
    "    \"width\": 200,\n",
    "    \"depth\": 5,\n",
    "}\n",
    "threshold = 50\n",
    "ktree = Ktree(threshold, data, Linf_simple, {}, {}, critic_args, device)\n",
    "ktree.create_tree_from_config(\"./models/line_segments/100/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad783ac7cc27a95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### k-tree stats\n",
    "\n",
    "Report some tree stats like the total number of nodes, height, number of leaves and leaf sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4a40365b1468a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:14:51.532109289Z",
     "start_time": "2024-01-10T12:14:51.518980823Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b39c3ea91b34e732",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "We utilise three types of accuracy measures.\n",
    "- accuracy of each individual Critic at each node\n",
    "- accuracy of the tree per layer for random query points\n",
    "- accuracy of the tree per layer for serialised query points\n",
    "\n",
    "The root node is indexed as \"0\" and every child's index is prefixed by the index of its parent followed by the \"child index\", e.g. the 2nd child of the root has index \"01\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ef55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = ktree.root.critic\n",
    "# get device of critic\n",
    "# critic is a neural network. get its device\n",
    "print(f\"Critic device: {next(critic.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71699aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.device)\n",
    "print(ktree.root.data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1baed9cacd8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:15:27.233999972Z",
     "start_time": "2024-01-10T12:14:51.527129818Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# np.random.seed(0)\n",
    "n = 100\n",
    "random_points = torch.zeros(n, ktree.dim)\n",
    "space = ktree.root.get_bounding_box()  # get the bounding box of the tree\n",
    "for i in range(n):\n",
    "    random_points[i] = torch.tensor(\n",
    "        # [np.random.uniform(space[d][0], space[d][1]) for d in range(ktree.dim)]\n",
    "        [\n",
    "            torch.rand(1) * (space[d][1] - space[d][0]) + space[d][0]\n",
    "            for d in range(ktree.dim)\n",
    "        ]\n",
    "    )\n",
    "# pass points on device\n",
    "# make them torch\n",
    "random_points = random_points.to(device)\n",
    "st_acc = ktree.get_critic_accuracies(random_points)\n",
    "\n",
    "print(st_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3257e5c8e338d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:30:38.730110485Z",
     "start_time": "2024-01-10T12:15:27.231556357Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc.random_queries(ktree, n=300, times=4, k=3)\n",
    "acc.serialised_queries(ktree, n=100, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7a43636b8a00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:34:12.270819369Z",
     "start_time": "2024-01-10T12:30:38.730141466Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc.serialised_queries(ktree, n=100, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
