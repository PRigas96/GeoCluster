{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8072660e8d1bf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GeoCluster Demo\n",
    "\n",
    "This notebook contains a demo of the GeoCluster algorithm for a dataset of **1000 squares** (with rotation) in 2D space.\n",
    "\n",
    "The metric used is `L_inf`, where the function is adjusted to calculate the `L_inf` distance between a square object and a point.\n",
    "\n",
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86065baf8506e9a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:05:09.386053369Z",
     "start_time": "2024-01-10T12:05:05.536599344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.k_tree import Ktree\n",
    "from src.k_tree_ls import Ktree as Ktree_ls\n",
    "from src.utils.objects.squares import loadData as loadSquares\n",
    "from src.metrics import Linf_simple\n",
    "from src.utils import plot_tools as pt\n",
    "from src.utils import accuracy as acc\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2f053c4d5f7275",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialise k-tree with training parameters\n",
    "\n",
    "In order to create the hierarchical clustering tree structure, we initialise a `Ktree` object which takes as parameters the training arguments for the *Clustering* and *Critic* models, as well as the arguments for the UN sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:05:09.387039580Z",
     "start_time": "2024-01-10T12:05:09.367373577Z"
    }
   },
   "outputs": [],
   "source": [
    "dim = 2  # space dimension\n",
    "\n",
    "k = 3  # number of centroids to generate in the Clustering model\n",
    "clustering_args = {\n",
    "    \"epochs\": 30,  # number of epochs\n",
    "    \"pre_processing\": 10,\n",
    "    \"number_of_centroids\": k,  # number of centroids to generate in the Clustering model\n",
    "    \"dimension\": dim,  # space dimension\n",
    "}\n",
    "n = 30\n",
    "un_args = {\n",
    "    \"N\": n,  # number of points to sample\n",
    "    \"M\": n**2 - 1,  # number of points to return\n",
    "    \"epsilon\": 0.15,  # the epsilon ball\n",
    "}\n",
    "critic_args = {\n",
    "    \"optimizer_lr\": 5e-3,  # optimiser learning rate\n",
    "    \"epochs\": 1000,  # number of epochs\n",
    "    \"width\": 200,  # width of the model's linear layers\n",
    "    \"depth\": 5,  # depth of the model's linear layers\n",
    "}\n",
    "\n",
    "threshold = 300  # if a tree node has data less than the threshold, stop division\n",
    "\n",
    "# Initialise the k-tree structure.\n",
    "ktree = Ktree_ls(\n",
    "    threshold, data2d, Linf_simple, clustering_args, un_args, critic_args, device, dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebb802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3D line_segments\n",
    "# n = 10000\n",
    "# lim = 50000\n",
    "# var = 500\n",
    "n = 100\n",
    "lim = 500\n",
    "var = 100\n",
    "\n",
    "bbox = [0, lim, 0, lim, 0, lim]\n",
    "length_size = [0.1, var]\n",
    "theta_size = [0, 180]\n",
    "phi_size = [0, 360]\n",
    "\n",
    "\n",
    "def create_line_segments_3d(n, bbox, length_size, theta_size, phi_size):\n",
    "    line_segments = torch.zeros((n, 6))  # ls = [x0,y0,z0, l, theta, phi]\n",
    "    for i in range(n):\n",
    "        x0 = torch.rand(1) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "        y0 = torch.rand(1) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "        z0 = torch.rand(1) * (bbox[5] - bbox[4]) + bbox[4]\n",
    "        l = torch.rand(1) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "        theta = torch.rand(1) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "        theta = torch.deg2rad(theta)\n",
    "        phi = torch.rand(1) * (phi_size[1] - phi_size[0]) + phi_size[0]\n",
    "        phi = torch.deg2rad(phi)\n",
    "        line_segments[i, :] = torch.cat([x0, y0, z0, l, theta, phi])\n",
    "    return line_segments\n",
    "\n",
    "\n",
    "data = create_line_segments_3d(n, bbox, length_size, theta_size, phi_size)\n",
    "# make the points\n",
    "data_points = torch.zeros((n, 6))\n",
    "for i in range(n):\n",
    "    data_points[i, 0] = data[i, 0]\n",
    "    data_points[i, 1] = data[i, 1]\n",
    "    data_points[i, 2] = data[i, 2]\n",
    "    data_points[i, 3] = data[i, 0] + data[i, 3] * torch.sin(data[i, 4]) * torch.cos(\n",
    "        data[i, 5]\n",
    "    )\n",
    "    data_points[i, 4] = data[i, 1] + data[i, 3] * torch.sin(data[i, 4]) * torch.sin(\n",
    "        data[i, 5]\n",
    "    )\n",
    "    data_points[i, 5] = data[i, 2] + data[i, 3] * torch.cos(data[i, 4])\n",
    "data3d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d859be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot them as 3d lines using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "for i in range(n):\n",
    "    ax.plot(\n",
    "        [data_points[i, 0], data_points[i, 3]],\n",
    "        [data_points[i, 1], data_points[i, 4]],\n",
    "        [data_points[i, 2], data_points[i, 5]],\n",
    "    )\n",
    "ax.set_xlim([bbox[0], bbox[1]])\n",
    "ax.set_ylim([bbox[2], bbox[3]])\n",
    "ax.set_zlim([bbox[4], bbox[5]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc41e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# check if 2 linesegments intersect\n",
    "\n",
    "\n",
    "def check_ls_intersection_2d(ls1, ls2):\n",
    "\n",
    "    # ls are of the form [x0,y0,l,theta]\n",
    "\n",
    "    x1 = ls1[0]\n",
    "\n",
    "    y1 = ls1[1]\n",
    "    l1 = ls1[2]\n",
    "\n",
    "\n",
    "    theta1 = ls1[3]\n",
    "\n",
    "    x2 = ls2[0]\n",
    "\n",
    "    y2 = ls2[1]\n",
    "    l2 = ls2[2]\n",
    "\n",
    "\n",
    "    theta2 = ls2[3]\n",
    "\n",
    "    # check if the lines are parallel\n",
    "\n",
    "    if torch.abs(theta1 - theta2) < 1e-6:\n",
    "\n",
    "        return False\n",
    "\n",
    "    # get their distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate #n line segments inside a bbox\n",
    "n = 100\n",
    "bbox = [0, 1000, 0, 1000]  # bbox\n",
    "length_size = [0.1, 100]  # length of the line segment\n",
    "theta_size = [0, 180]  # angle of the line segment\n",
    "\n",
    "\n",
    "# line segments will be [x0,y0,l,theta]\n",
    "# sample x0,y0 inside bbox\n",
    "def create_line_segments(n, bbox, length_size, theta_size):\n",
    "    print(f\"Creating {n} line segments\")\n",
    "    x0 = torch.rand(n) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "    y0 = torch.rand(n) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "    print(f\"x0 shape: {x0.shape}\")\n",
    "    # sample l inside length_size\n",
    "    l = torch.rand(n) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "    print(f\"l shape: {l.shape}\")\n",
    "    # sample theta inside theta_size\n",
    "    theta_size = torch.rand(n) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "    print(f\"theta shape: {theta_size.shape}\")\n",
    "    theta_size = torch.deg2rad(theta_size)\n",
    "    return torch.stack([x0, y0, l, theta_size], dim=1)\n",
    "\n",
    "\n",
    "data = create_line_segments(n, bbox, length_size, theta_size)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "# first make data from embedding to datapoints\n",
    "data_points = torch.zeros(n, 4)\n",
    "data_points[:, 0] = data[:, 0]\n",
    "data_points[:, 1] = data[:, 1]\n",
    "data_points[:, 2] = data[:, 0] + data[:, 2] * torch.cos(data[:, 3])\n",
    "data_points[:, 3] = data[:, 1] + data[:, 2] * torch.sin(data[:, 3])\n",
    "print(f\"Data points shape: {data_points.shape}\")\n",
    "data2d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([data_points[:, 0], data_points[:, 2]], [data_points[:, 1], data_points[:, 3]])\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(100, 4)\n",
    "a, b, c, d = (\n",
    "    x[:, 0],\n",
    "    x[:, 1],\n",
    "    x[:, 2],\n",
    "    x[:, 3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_distances(segments, centroid):\n",
    "    print(segments.shape)\n",
    "    print(type(segments))\n",
    "    print(segments[0])\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, lengths, thetas = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.cos(thetas)\n",
    "    y1s = y0s + lengths * torch.sin(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt((nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "segments = data\n",
    "centroid = torch.tensor([2.0, 2.0])  # [x0, y0]\n",
    "\n",
    "distances = compute_distances(segments, centroid)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00930561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_distances_2d(segments, centroid):\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, lengths, thetas = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.cos(thetas)\n",
    "    y1s = y0s + lengths * torch.sin(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt((nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def compute_distances_3d(segments, centroid):\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy, cz = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, z0s, lengths, thetas, phis = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "        segments[:, 4],\n",
    "        segments[:, 5],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.sin(thetas) * torch.cos(phis)\n",
    "    y1s = y0s + lengths * torch.sin(thetas) * torch.sin(phis)\n",
    "    z1s = z0s + lengths * torch.cos(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s, cz - z0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s, z1s - z0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    nearest_zs = z0s + projection_scalars * (z1s - z0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt(\n",
    "        (nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2 + (nearest_zs - cz) ** 2\n",
    "    )\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def get_dist_matrix(data, centroids, dist_function):\n",
    "    # init.\n",
    "    dist_matrix = torch.zeros(data.shape[0], centroids.shape[0])\n",
    "    for i in range(centroids.shape[0]):\n",
    "        dist_matrix[:, i] = dist_function(data, centroids[i])\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def kmeans_pp_greedy(\n",
    "    data,\n",
    "    n_clusters,\n",
    "    dist_function,\n",
    "    dim,\n",
    "    random_state=None,\n",
    "    n_trials=None,\n",
    "):\n",
    "    # check parameters\n",
    "    if type(data) is not torch.Tensor:\n",
    "        data = torch.tensor(data)\n",
    "    n_samples, n_features = data.shape\n",
    "    if n_clusters > n_samples:\n",
    "        raise ValueError(\n",
    "            \"n_clusters should be smaller or equal to the number of centroids\"\n",
    "        )\n",
    "    if type(n_clusters) is not torch.Tensor:\n",
    "        n_clusters = torch.tensor(n_clusters)\n",
    "    if n_trials is None:\n",
    "        n_trials = 2 + int(torch.log(n_clusters))\n",
    "    # set random state\n",
    "    if random_state is not None:\n",
    "        torch.manual_seed(random_state)\n",
    "    # initialize centroids\n",
    "    centroids = torch.zeros(n_clusters, dim)\n",
    "    # choose first centroid\n",
    "    first_centroid_idx = torch.randint(n_samples, (1,))\n",
    "    # print(f\"First centroid index: {first_centroid_idx}\")\n",
    "    data_oi = data[first_centroid_idx]\n",
    "    if dim == 2:\n",
    "        x0, y0, l, theta = data_oi[0]\n",
    "        centroids[0] = torch.tensor([x0, y0])\n",
    "        centroids[0] += torch.tensor([0.5 * l * torch.cos(theta), l * torch.sin(theta)])\n",
    "    elif dim == 3:\n",
    "        x0, y0, z0, l, theta, phi = data_oi[0]\n",
    "        centroids[0] = torch.tensor([x0, y0, z0])\n",
    "        centroids[0] += torch.tensor(\n",
    "            [\n",
    "                0.5 * l * torch.sin(theta) * torch.cos(phi),\n",
    "                0.5 * l * torch.sin(theta) * torch.sin(phi),\n",
    "                0.5 * l * torch.cos(theta),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"dim should be 2 or 3\")\n",
    "\n",
    "    # create a vector of minus ones of shape (n_samples,)\n",
    "    indices = -torch.ones(n_samples)\n",
    "    # init dist matrix\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Centroids shape: {centroids.shape}\")\n",
    "    print(f\"Dist function: {dist_function}\")\n",
    "    dist_matrix = get_dist_matrix(data, centroids[:1], dist_function)\n",
    "\n",
    "    for i in range(1, n_clusters):\n",
    "        # print(\"=\" * 20)\n",
    "        # print(\"=\" * 20)\n",
    "        # choose the next centroid\n",
    "        freq = torch.zeros(n_trials)\n",
    "        for _ in range(n_trials):\n",
    "            # choose a centroid with probability proportional to the distance\n",
    "            # to the closest centroid\n",
    "            dists = torch.min(dist_matrix, dim=1).values\n",
    "            probs = dists / torch.sum(dists)\n",
    "            # print(f\"Probs : {probs}\")\n",
    "            next_centroid_idx = torch.multinomial(probs, 1)\n",
    "            # print(f\"Next centroid index: {next_centroid_idx}\")\n",
    "            # update best_dist\n",
    "            freq[_] = next_centroid_idx\n",
    "        # pick randomly a vlaue from freq\n",
    "        idx = torch.randint(n_trials, (1,))\n",
    "        next_centroid_idx = freq[idx].int()\n",
    "        # print(\"=\" * 20)\n",
    "        # print(f\"Next centroid index: {next_centroid_idx}\")\n",
    "        # print(f\"Next data: {data[next_centroid_idx]}\")\n",
    "        # print(\"=\" * 20)\n",
    "        # choose the centroid with the smallest best_dist\n",
    "        data_oi = data[next_centroid_idx]\n",
    "        if dim == 2:\n",
    "            x0, y0, l, theta = data_oi[0]\n",
    "            centroids[i] = torch.tensor([x0, y0])\n",
    "            centroids[i] += torch.tensor(\n",
    "                [0.5 * l * torch.cos(theta), l * torch.sin(theta)]\n",
    "            )\n",
    "        elif dim == 3:\n",
    "            x0, y0, z0, l, theta, phi = data_oi[0]\n",
    "            centroids[i] = torch.tensor([x0, y0, z0])\n",
    "            centroids[i] += torch.tensor(\n",
    "                [\n",
    "                    0.5 * l * torch.sin(theta) * torch.cos(phi),\n",
    "                    0.5 * l * torch.sin(theta) * torch.sin(phi),\n",
    "                    0.5 * l * torch.cos(theta),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"dim should be 2 or 3\")\n",
    "\n",
    "        # update dist_matrix\n",
    "        dist_matrix = get_dist_matrix(data, centroids[: i + 1], dist_function)\n",
    "        # print(dist_matrix.shape)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_2d(segments, centroid):\n",
    "    # Unpack the centroid coordinates\n",
    "    cx, cy = centroid\n",
    "    # Unpack the segments\n",
    "    x0s, y0s, lengths, thetas = (\n",
    "        segments[:, 0],\n",
    "        segments[:, 1],\n",
    "        segments[:, 2],\n",
    "        segments[:, 3],\n",
    "    )\n",
    "    # Calculate the endpoints of the segments\n",
    "    x1s = x0s + lengths * torch.cos(thetas)\n",
    "    y1s = y0s + lengths * torch.sin(thetas)\n",
    "    # Vector from (x0, y0) to centroid\n",
    "    vec_p0_c = torch.stack([cx - x0s, cy - y0s], dim=1)\n",
    "    # Direction vector of the segments\n",
    "    vec_p0_p1 = torch.stack([x1s - x0s, y1s - y0s], dim=1)\n",
    "    # Projection scalar of vec_p0_c onto vec_p0_p1\n",
    "    dot_products = torch.sum(vec_p0_c * vec_p0_p1, dim=1)\n",
    "    segment_lengths_squared = torch.sum(vec_p0_p1 * vec_p0_p1, dim=1)\n",
    "    projection_scalars = dot_products / segment_lengths_squared\n",
    "    # Clamp the projection_scalars to lie within the segment\n",
    "    projection_scalars = torch.clamp(projection_scalars, min=0, max=1)\n",
    "    # Calculate the nearest points on the segments to the centroid\n",
    "    nearest_xs = x0s + projection_scalars * (x1s - x0s)\n",
    "    nearest_ys = y0s + projection_scalars * (y1s - y0s)\n",
    "    # Distance from nearest points on the segments to the centroid\n",
    "    distances = torch.sqrt((nearest_xs - cx) ** 2 + (nearest_ys - cy) ** 2)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "centroid = torch.tensor([393, 228])  # [x0, y0]\n",
    "# lets plot them\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segments = data2d[0]\n",
    "plt.plot(\n",
    "    [segments[0], segments[0] + segments[2] * torch.cos(segments[3])],\n",
    "    [segments[1], segments[1] + segments[2] * torch.sin(segments[3])],\n",
    ")\n",
    "plt.scatter(centroid[0], centroid[1], color=\"r\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "# find their distance\n",
    "segments = data2d[:1]\n",
    "print(segments)\n",
    "distances = compute_distances_2d(segments, centroid)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f17aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLS:\n",
    "    def __init__(self, data, k, dim, dist_function):\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "        self.dist_function = dist_function\n",
    "\n",
    "    def get_points_from_emb(self, data, dim):\n",
    "        \"\"\"\n",
    "        Get the points from the embedding\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch.Tensor\n",
    "            The data tensor\n",
    "        dim : int\n",
    "            The dimension of the space\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The points in the space [x0,y0,x1,y1,...]\n",
    "        \"\"\"\n",
    "        data_points = torch.zeros(data.shape[0], 2 * dim)\n",
    "        if dim == 2:  # [x0,y0,l,theta]\n",
    "            data_points[:, 0] = data[:, 0]\n",
    "            data_points[:, 1] = data[:, 1]\n",
    "            data_points[:, 2] = data[:, 0] + data[:, 2] * torch.cos(data[:, 3])\n",
    "            data_points[:, 3] = data[:, 1] + data[:, 2] * torch.sin(data[:, 3])\n",
    "        elif dim == 3:  # [x0,y0,z0,l,theta,phi]\n",
    "            data_points[:, 0] = data[:, 0]\n",
    "            data_points[:, 1] = data[:, 1]\n",
    "            data_points[:, 2] = data[:, 2]\n",
    "            data_points[:, 3] = data[:, 0] + data[:, 3] * torch.sin(\n",
    "                data[:, 4]\n",
    "            ) * torch.cos(data[:, 5])\n",
    "            data_points[:, 4] = data[:, 1] + data[:, 3] * torch.sin(\n",
    "                data[:, 4]\n",
    "            ) * torch.sin(data[:, 5])\n",
    "            data_points[:, 5] = data[:, 2] + data[:, 3] * torch.cos(data[:, 4])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"dim should be 2 or 3\")\n",
    "        return data_points\n",
    "\n",
    "    def fit(self, n_iter, n_trials=10):\n",
    "        self.data_points = self.get_points_from_emb(self.data, self.dim)\n",
    "        # initialize using kmeans++ greedy\n",
    "        centroids = kmeans_pp_greedy(\n",
    "            self.data, self.k, self.dist_function, self.dim, n_trials=n_trials\n",
    "        )\n",
    "        # get initial divergence\n",
    "        dist_matrix = get_dist_matrix(self.data, centroids, self.dist_function)\n",
    "        div = torch.sum(torch.min(dist_matrix, dim=1).values)\n",
    "        print(f\"Initial divergence: {div}\")\n",
    "\n",
    "        # do the iterations\n",
    "        for i in range(n_iter):\n",
    "            # get the distances\n",
    "            dist_matrix = get_dist_matrix(self.data, centroids, self.dist_function)\n",
    "            # get the labels\n",
    "            labels = torch.argmin(dist_matrix, dim=1)\n",
    "            # update the centroids\n",
    "            for j in range(self.k):\n",
    "                dp = self.data_points[labels == j]\n",
    "                centroids[j][0] = torch.mean(torch.concatenate([dp[:, 0], dp[:, 2]]))\n",
    "                centroids[j][1] = torch.mean(torch.concatenate([dp[:, 1], dp[:, 3]]))\n",
    "                if self.dim == 3:\n",
    "                    centroids[j][2] = torch.mean(\n",
    "                        torch.concatenate([dp[:, 2], dp[:, 4]])\n",
    "                    )\n",
    "            # get the divergence\n",
    "            div = torch.sum(torch.min(dist_matrix, dim=1).values)\n",
    "            print(f\"Iteration {i+1}, divergence: {div}\")\n",
    "        self.centroids = centroids\n",
    "\n",
    "    def predict(self, centroids):\n",
    "        # get the distances\n",
    "        dist_matrix = get_dist_matrix(self.data, centroids, self.dist_function)\n",
    "        # get the labels\n",
    "        labels = torch.argmin(dist_matrix, dim=1)\n",
    "        return labels\n",
    "\n",
    "\n",
    "# kmeans = ClusteringLS(data, 3, 2, compute_distances_2d)\n",
    "# kmeans.fit(10)\n",
    "# centroids = kmeans.centroids\n",
    "# plt.plot([data_points[:, 0], data_points[:, 2]], [data_points[:, 1], data_points[:, 3]])\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1])\n",
    "# plt.axis(\"equal\")\n",
    "# plt.show()\n",
    "kmeans = ClusteringLS(data3d, 3, 3, compute_distances_3d)\n",
    "kmeans.fit(10)\n",
    "centroids = kmeans.centroids\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "for i in range(kmeans.data_points.shape[0]):\n",
    "    ax.plot(\n",
    "        [kmeans.data_points[i, 0], kmeans.data_points[i, 3]],\n",
    "        [kmeans.data_points[i, 1], kmeans.data_points[i, 4]],\n",
    "        [kmeans.data_points[i, 2], kmeans.data_points[i, 5]],\n",
    "    )\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c=\"r\", s=100)\n",
    "ax.set_xlim([bbox[0], bbox[1]])\n",
    "ax.set_ylim([bbox[2], bbox[3]])\n",
    "ax.set_zlim([bbox[4], bbox[5]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5763086fb5664",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create the k-tree\n",
    "\n",
    "### Train the k-tree\n",
    "\n",
    "Starting from the tree's root node, train the *Clustering* and *Critic* models, store the Critic model on the node and assign the data on its children according to the Critic's predictions.\n",
    "Repeat recursively the process until a node has less amount of data than the threshold.\n",
    "\n",
    "Also plot some training results and store them as well as the resulting models to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53242dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 100 line segments\n",
      "x0 shape: torch.Size([100])\n",
      "l shape: torch.Size([100])\n",
      "theta shape: torch.Size([100])\n",
      "Data shape: torch.Size([100, 4])\n",
      "Data points shape: torch.Size([100, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.k_tree import Ktree\n",
    "from src.k_tree_ls import Ktree as Ktree_ls\n",
    "from src.utils.objects.squares import loadData as loadSquares\n",
    "from src.metrics import Linf_simple\n",
    "from src.utils import plot_tools as pt\n",
    "from src.utils import accuracy as acc\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# lets generate #n line segments inside a bbox\n",
    "n = 100\n",
    "bbox = [0, 1000, 0, 1000]  # bbox\n",
    "length_size = [0.1, 100]  # length of the line segment\n",
    "theta_size = [0, 180]  # angle of the line segment\n",
    "\n",
    "\n",
    "# line segments will be [x0,y0,l,theta]\n",
    "# sample x0,y0 inside bbox\n",
    "def create_line_segments(n, bbox, length_size, theta_size):\n",
    "    print(f\"Creating {n} line segments\")\n",
    "    x0 = torch.rand(n) * (bbox[1] - bbox[0]) + bbox[0]\n",
    "    y0 = torch.rand(n) * (bbox[3] - bbox[2]) + bbox[2]\n",
    "    print(f\"x0 shape: {x0.shape}\")\n",
    "    # sample l inside length_size\n",
    "    l = torch.rand(n) * (length_size[1] - length_size[0]) + length_size[0]\n",
    "    print(f\"l shape: {l.shape}\")\n",
    "    # sample theta inside theta_size\n",
    "    theta_size = torch.rand(n) * (theta_size[1] - theta_size[0]) + theta_size[0]\n",
    "    print(f\"theta shape: {theta_size.shape}\")\n",
    "    theta_size = torch.deg2rad(theta_size)\n",
    "    return torch.stack([x0, y0, l, theta_size], dim=1)\n",
    "\n",
    "\n",
    "data = create_line_segments(n, bbox, length_size, theta_size)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "# first make data from embedding to datapoints\n",
    "data_points = torch.zeros(n, 4)\n",
    "data_points[:, 0] = data[:, 0]\n",
    "data_points[:, 1] = data[:, 1]\n",
    "data_points[:, 2] = data[:, 0] + data[:, 2] * torch.cos(data[:, 3])\n",
    "data_points[:, 3] = data[:, 1] + data[:, 2] * torch.sin(data[:, 3])\n",
    "print(f\"Data points shape: {data_points.shape}\")\n",
    "data2d = data\n",
    "\n",
    "dim = 2  # space dimension\n",
    "\n",
    "k = 3  # number of centroids to generate in the Clustering model\n",
    "clustering_args = {\n",
    "    \"epochs\": 30,  # number of epochs\n",
    "    \"pre_processing\": 10,\n",
    "    \"number_of_centroids\": k,  # number of centroids to generate in the Clustering model\n",
    "    \"dimension\": dim,  # space dimension\n",
    "}\n",
    "n = 30\n",
    "un_args = {\n",
    "    \"N\": n,  # number of points to sample\n",
    "    \"M\": n**2 - 1,  # number of points to return\n",
    "    \"epsilon\": 0.15,  # the epsilon ball\n",
    "}\n",
    "critic_args = {\n",
    "    \"optimizer_lr\": 5e-3,  # optimiser learning rate\n",
    "    \"epochs\": 2000,  # number of epochs\n",
    "    \"width\": 200,  # width of the model's linear layers\n",
    "    \"depth\": 5,  # depth of the model's linear layers\n",
    "}\n",
    "\n",
    "threshold = 20  # if a tree node has data less than the threshold, stop division\n",
    "\n",
    "# Initialise the k-tree structure.\n",
    "ktree = Ktree_ls(\n",
    "    threshold, data2d, Linf_simple, clustering_args, un_args, critic_args, device, dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0983f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Creating critic for node 0 that has 100 data, which is more than the threshold 20.\n",
      "Bounding box for node 0: [[tensor(-49.8738), tensor(1068.2593)], [tensor(-0.0372), tensor(976.8906)]]\n",
      "Creating clustering for node 0 with 3 centroids.\n",
      "Initial divergence: 24944.404296875\n",
      "Iteration 1, divergence: 24944.404296875\n",
      "Iteration 2, divergence: 24334.017578125\n",
      "Iteration 3, divergence: 23736.716796875\n",
      "Iteration 4, divergence: 22042.62890625\n",
      "Iteration 5, divergence: 21473.44921875\n",
      "Iteration 6, divergence: 21492.447265625\n",
      "Iteration 7, divergence: 21413.337890625\n",
      "Iteration 8, divergence: 21272.08203125\n",
      "Iteration 9, divergence: 21027.123046875\n",
      "Iteration 10, divergence: 20979.10546875\n",
      "Iteration 11, divergence: 20979.10546875\n",
      "Iteration 12, divergence: 20979.10546875\n",
      "Iteration 13, divergence: 20979.10546875\n",
      "Iteration 14, divergence: 20979.10546875\n",
      "Iteration 15, divergence: 20979.10546875\n",
      "Iteration 16, divergence: 20979.10546875\n",
      "Iteration 17, divergence: 20979.10546875\n",
      "Iteration 18, divergence: 20979.10546875\n",
      "Iteration 19, divergence: 20979.10546875\n",
      "Iteration 20, divergence: 20979.10546875\n",
      "Iteration 21, divergence: 20979.10546875\n",
      "Iteration 22, divergence: 20979.10546875\n",
      "Iteration 23, divergence: 20979.10546875\n",
      "Iteration 24, divergence: 20979.10546875\n",
      "Iteration 25, divergence: 20979.10546875\n",
      "Iteration 26, divergence: 20979.10546875\n",
      "Iteration 27, divergence: 20979.10546875\n",
      "Iteration 28, divergence: 20979.10546875\n",
      "Iteration 29, divergence: 20979.10546875\n",
      "Iteration 30, divergence: 20979.10546875\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[328.61057 231.30894]\n",
      " [332.69714 740.2378 ]\n",
      " [813.8053  448.91098]]\n",
      "scale is 1118.133056640625\n",
      "Processing...\n",
      "flag is 583\n",
      "m is 318\n",
      "i is 900\n",
      "Labeled 0/318 points.\n",
      "Labeled all 318/318 points.\n",
      "Creating critic for node 0 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.27672955974842767\n",
      "Epoch:  0 Cost:  978.5607299804688\n",
      "Acc:  0.7515723270440252\n",
      "Epoch:  200 Cost:  50.546974182128906\n",
      "Acc:  0.7955974842767296\n",
      "Epoch:  400 Cost:  35.5106315612793\n",
      "Acc:  0.839622641509434\n",
      "Epoch:  600 Cost:  40.86474609375\n",
      "Acc:  0.8553459119496856\n",
      "Epoch:  800 Cost:  30.892574310302734\n",
      "Acc:  0.9591194968553459\n",
      "Epoch:  1000 Cost:  15.06671142578125\n",
      "Acc:  0.9276729559748428\n",
      "Epoch:  1200 Cost:  17.54517936706543\n",
      "Acc:  0.5754716981132075\n",
      "Epoch:  1400 Cost:  81.23883819580078\n",
      "Acc:  0.7704402515723271\n",
      "Epoch:  1600 Cost:  50.22065734863281\n",
      "Acc:  0.8459119496855346\n",
      "Epoch:  1800 Cost:  45.96524429321289\n",
      "Saved critic config to ./models/line_segments/100/demo0_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/demo0_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 00 that has 33 data, which is more than the threshold 20.\n",
      "Bounding box for node 00: [[tensor(-49.8738), tensor(722.7009)], [tensor(7.7844), tensor(439.3817)]]\n",
      "Creating clustering for node 00 with 3 centroids.\n",
      "Initial divergence: 3154.59912109375\n",
      "Iteration 1, divergence: 3154.59912109375\n",
      "Iteration 2, divergence: 3038.160888671875\n",
      "Iteration 3, divergence: 3043.9833984375\n",
      "Iteration 4, divergence: 3055.19384765625\n",
      "Iteration 5, divergence: 3055.19384765625\n",
      "Iteration 6, divergence: 3055.19384765625\n",
      "Iteration 7, divergence: 3055.19384765625\n",
      "Iteration 8, divergence: 3055.19384765625\n",
      "Iteration 9, divergence: 3055.19384765625\n",
      "Iteration 10, divergence: 3055.19384765625\n",
      "Iteration 11, divergence: 3055.19384765625\n",
      "Iteration 12, divergence: 3055.19384765625\n",
      "Iteration 13, divergence: 3055.19384765625\n",
      "Iteration 14, divergence: 3055.19384765625\n",
      "Iteration 15, divergence: 3055.19384765625\n",
      "Iteration 16, divergence: 3055.19384765625\n",
      "Iteration 17, divergence: 3055.19384765625\n",
      "Iteration 18, divergence: 3055.19384765625\n",
      "Iteration 19, divergence: 3055.19384765625\n",
      "Iteration 20, divergence: 3055.19384765625\n",
      "Iteration 21, divergence: 3055.19384765625\n",
      "Iteration 22, divergence: 3055.19384765625\n",
      "Iteration 23, divergence: 3055.19384765625\n",
      "Iteration 24, divergence: 3055.19384765625\n",
      "Iteration 25, divergence: 3055.19384765625\n",
      "Iteration 26, divergence: 3055.19384765625\n",
      "Iteration 27, divergence: 3055.19384765625\n",
      "Iteration 28, divergence: 3055.19384765625\n",
      "Iteration 29, divergence: 3055.19384765625\n",
      "Iteration 30, divergence: 3055.19384765625\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[335.33777 186.17383]\n",
      " [ 99.29993 264.9507 ]\n",
      " [551.194   242.8023 ]]\n",
      "scale is 772.57470703125\n",
      "Processing...\n",
      "flag is 411\n",
      "m is 490\n",
      "i is 900\n",
      "Labeled 0/490 points.\n",
      "Labeled all 490/490 points.\n",
      "Creating critic for node 00 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.3816326530612245\n",
      "Epoch:  0 Cost:  308.86572265625\n",
      "Acc:  0.8367346938775511\n",
      "Epoch:  200 Cost:  40.823997497558594\n",
      "Acc:  0.8816326530612245\n",
      "Epoch:  400 Cost:  31.76228141784668\n",
      "Acc:  0.963265306122449\n",
      "Epoch:  600 Cost:  15.10947036743164\n",
      "Acc:  0.9346938775510204\n",
      "Epoch:  800 Cost:  16.673316955566406\n",
      "Acc:  0.9857142857142858\n",
      "Epoch:  1000 Cost:  10.186897277832031\n",
      "Acc:  0.9795918367346939\n",
      "Epoch:  1200 Cost:  9.183680534362793\n",
      "Acc:  0.8653061224489796\n",
      "Epoch:  1400 Cost:  46.452125549316406\n",
      "Acc:  0.9040816326530612\n",
      "Epoch:  1600 Cost:  34.76182174682617\n",
      "Acc:  0.9224489795918367\n",
      "Epoch:  1800 Cost:  29.051048278808594\n",
      "Saved critic config to ./models/line_segments/100/demo00_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/demo00_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 01 that has 34 data, which is more than the threshold 20.\n",
      "Bounding box for node 01: [[tensor(-42.1157), tensor(696.0206)], [tensor(497.7448), tensor(964.0361)]]\n",
      "Creating clustering for node 01 with 3 centroids.\n",
      "Initial divergence: 6093.3505859375\n",
      "Iteration 1, divergence: 6093.3505859375\n",
      "Iteration 2, divergence: 4449.59326171875\n",
      "Iteration 3, divergence: 4291.07958984375\n",
      "Iteration 4, divergence: 4131.2939453125\n",
      "Iteration 5, divergence: 4131.2939453125\n",
      "Iteration 6, divergence: 4131.2939453125\n",
      "Iteration 7, divergence: 4131.2939453125\n",
      "Iteration 8, divergence: 4131.2939453125\n",
      "Iteration 9, divergence: 4131.2939453125\n",
      "Iteration 10, divergence: 4131.2939453125\n",
      "Iteration 11, divergence: 4131.2939453125\n",
      "Iteration 12, divergence: 4131.2939453125\n",
      "Iteration 13, divergence: 4131.2939453125\n",
      "Iteration 14, divergence: 4131.2939453125\n",
      "Iteration 15, divergence: 4131.2939453125\n",
      "Iteration 16, divergence: 4131.2939453125\n",
      "Iteration 17, divergence: 4131.2939453125\n",
      "Iteration 18, divergence: 4131.2939453125\n",
      "Iteration 19, divergence: 4131.2939453125\n",
      "Iteration 20, divergence: 4131.2939453125\n",
      "Iteration 21, divergence: 4131.2939453125\n",
      "Iteration 22, divergence: 4131.2939453125\n",
      "Iteration 23, divergence: 4131.2939453125\n",
      "Iteration 24, divergence: 4131.2939453125\n",
      "Iteration 25, divergence: 4131.2939453125\n",
      "Iteration 26, divergence: 4131.2939453125\n",
      "Iteration 27, divergence: 4131.2939453125\n",
      "Iteration 28, divergence: 4131.2939453125\n",
      "Iteration 29, divergence: 4131.2939453125\n",
      "Iteration 30, divergence: 4131.2939453125\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[190.40025 856.00616]\n",
      " [215.74532 583.0328 ]\n",
      " [475.35648 797.66766]]\n",
      "scale is 738.1362915039062\n",
      "Processing...\n",
      "flag is 543\n",
      "m is 358\n",
      "i is 900\n",
      "Labeled 0/358 points.\n",
      "Labeled all 358/358 points.\n",
      "Creating critic for node 01 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.36033519553072624\n",
      "Epoch:  0 Cost:  499.9784851074219\n",
      "Acc:  0.7262569832402235\n",
      "Epoch:  200 Cost:  49.9216423034668\n",
      "Acc:  0.8044692737430168\n",
      "Epoch:  400 Cost:  43.768714904785156\n",
      "Acc:  0.8296089385474861\n",
      "Epoch:  600 Cost:  39.698692321777344\n",
      "Acc:  0.8798882681564246\n",
      "Epoch:  800 Cost:  33.30104064941406\n",
      "Acc:  0.9078212290502793\n",
      "Epoch:  1000 Cost:  28.402475357055664\n",
      "Acc:  0.9106145251396648\n",
      "Epoch:  1200 Cost:  23.46250343322754\n",
      "Acc:  0.9497206703910615\n",
      "Epoch:  1400 Cost:  18.89516258239746\n",
      "Acc:  0.9553072625698324\n",
      "Epoch:  1600 Cost:  16.38564682006836\n",
      "Acc:  0.9664804469273743\n",
      "Epoch:  1800 Cost:  14.565786361694336\n",
      "Saved critic config to ./models/line_segments/100/demo01_critic_config.pt\n",
      "Saved critic training results to ./models/line_segments/100/demo01_critic_training_results.npy\n",
      "\n",
      "====================\n",
      "Creating critic for node 02 that has 33 data, which is more than the threshold 20.\n",
      "Bounding box for node 02: [[tensor(558.7831), tensor(1068.2593)], [tensor(-0.0372), tensor(976.8906)]]\n",
      "Creating clustering for node 02 with 3 centroids.\n",
      "Initial divergence: 4885.63037109375\n",
      "Iteration 1, divergence: 4885.63037109375\n",
      "Iteration 2, divergence: 4186.240234375\n",
      "Iteration 3, divergence: 3908.54345703125\n",
      "Iteration 4, divergence: 3784.7802734375\n",
      "Iteration 5, divergence: 3737.7548828125\n",
      "Iteration 6, divergence: 3737.7548828125\n",
      "Iteration 7, divergence: 3737.7548828125\n",
      "Iteration 8, divergence: 3737.7548828125\n",
      "Iteration 9, divergence: 3737.7548828125\n",
      "Iteration 10, divergence: 3737.7548828125\n",
      "Iteration 11, divergence: 3737.7548828125\n",
      "Iteration 12, divergence: 3737.7548828125\n",
      "Iteration 13, divergence: 3737.7548828125\n",
      "Iteration 14, divergence: 3737.7548828125\n",
      "Iteration 15, divergence: 3737.7548828125\n",
      "Iteration 16, divergence: 3737.7548828125\n",
      "Iteration 17, divergence: 3737.7548828125\n",
      "Iteration 18, divergence: 3737.7548828125\n",
      "Iteration 19, divergence: 3737.7548828125\n",
      "Iteration 20, divergence: 3737.7548828125\n",
      "Iteration 21, divergence: 3737.7548828125\n",
      "Iteration 22, divergence: 3737.7548828125\n",
      "Iteration 23, divergence: 3737.7548828125\n",
      "Iteration 24, divergence: 3737.7548828125\n",
      "Iteration 25, divergence: 3737.7548828125\n",
      "Iteration 26, divergence: 3737.7548828125\n",
      "Iteration 27, divergence: 3737.7548828125\n",
      "Iteration 28, divergence: 3737.7548828125\n",
      "Iteration 29, divergence: 3737.7548828125\n",
      "Iteration 30, divergence: 3737.7548828125\n",
      "====================\n",
      "getUncertaintyArea\n",
      "Centroids are [[872.1023  145.9157 ]\n",
      " [866.5432  733.3306 ]\n",
      " [702.77014 467.48672]]\n",
      "scale is 976.927734375\n",
      "Processing...\n",
      "flag is 525\n",
      "m is 376\n",
      "i is 900\n",
      "Labeled 0/376 points.\n",
      "Labeled all 376/376 points.\n",
      "Creating critic for node 02 with 3 centroids.\n",
      "Device is: cuda:0\n",
      "====================\n",
      "Training Critic Model\n",
      "Acc:  0.4015957446808511\n",
      "Epoch:  0 Cost:  1069.5653076171875\n",
      "Acc:  0.723404255319149\n",
      "Epoch:  200 Cost:  56.23839569091797\n",
      "Acc:  0.776595744680851\n",
      "Epoch:  400 Cost:  51.79665756225586\n",
      "Acc:  0.851063829787234\n",
      "Epoch:  600 Cost:  43.347347259521484\n",
      "Acc:  0.8776595744680851\n",
      "Epoch:  800 Cost:  39.38900375366211\n",
      "Acc:  0.7473404255319149\n",
      "Epoch:  1000 Cost:  44.1654052734375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mktree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/line_segments/100/demo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\Documents\\Code\\GeoCluster\\src\\k_tree_ls.py:109\u001b[0m, in \u001b[0;36mKtree.create_tree\u001b[1;34m(self, save_path_prefix, plot)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating critic for node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data, which is more than the threshold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path_index_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     node\u001b[38;5;241m.\u001b[39mdivide()\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\Documents\\Code\\GeoCluster\\src\\k_tree_ls.py:597\u001b[0m, in \u001b[0;36mKtree.Node.create_critic\u001b[1;34m(self, save_path_prefix, plot)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    595\u001b[0m critic \u001b[38;5;241m=\u001b[39m Critic(n_of_centroids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mktree\u001b[38;5;241m.\u001b[39mdim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mktree\u001b[38;5;241m.\u001b[39mdim, width, depth)\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# initialize the voronoi network\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m \u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcritic_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_lr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcritic_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m              \u001b[49m\u001b[43mqp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m              \u001b[49m\u001b[43mF_ps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_ps\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m critic\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    604\u001b[0m critic\u001b[38;5;241m.\u001b[39mload_state_dict(critic\u001b[38;5;241m.\u001b[39mbest_vor_model_state)\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\Documents\\Code\\GeoCluster\\src\\models.py:590\u001b[0m, in \u001b[0;36mCritic.train_\u001b[1;34m(self, optimizer, epochs, device, qp, F_ps, times)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m    589\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 590\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cost \u001b[38;5;241m<\u001b[39m best_vor_cost:\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ktree.create_tree(save_path_prefix=\"./models/line_segments/100/demo\", plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c94cd5651af86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:14:48.624832180Z",
     "start_time": "2024-01-10T12:05:09.378879820Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ktree.create_tree(save_path_prefix=\"./models/squares/1000/demo\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60626e8af3d794",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the k-tree from saved configuration\n",
    "\n",
    "For a k-tree instance that is already trained and saved to configuration files (all existing in the same path and having the same prefix), load the instance by simply specifying that path and prefix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a04a448f5e397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:14:51.507813809Z",
     "start_time": "2024-01-10T12:14:48.677600920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Critic's architecture arguments are still required.\n",
    "critic_args = {\n",
    "    \"width\": 200,\n",
    "    \"depth\": 5,\n",
    "}\n",
    "threshold = 300\n",
    "ktree = Ktree(threshold, data, Linf_simple, {}, {}, critic_args, device)\n",
    "ktree.create_tree_from_config(\"./models/squares/1000/demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad783ac7cc27a95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### k-tree stats\n",
    "\n",
    "Report some tree stats like the total number of nodes, height, number of leaves and leaf sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e4a40365b1468a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:14:51.532109289Z",
     "start_time": "2024-01-10T12:14:51.518980823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree node number is 4\n",
      "Tree height is 2.\n",
      "Created 3 leaves with sizes\n",
      "[32, 33, 35]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tree node number is {ktree.number_of_nodes}\")\n",
    "\n",
    "leaves = ktree.get_leaves()\n",
    "\n",
    "height = max([len(leaf.index) for leaf in leaves])\n",
    "print(f\"Tree height is {height}.\")\n",
    "\n",
    "leaf_sizes = [len(leaf.data) for leaf in leaves]\n",
    "print(f\"Created {len(leaves)} leaves with sizes\")\n",
    "print(leaf_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c3ea91b34e732",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "We utilise three types of accuracy measures.\n",
    "- accuracy of each individual Critic at each node\n",
    "- accuracy of the tree per layer for random query points\n",
    "- accuracy of the tree per layer for serialised query points\n",
    "\n",
    "The root node is indexed as \"0\" and every child's index is prefixed by the index of its parent followed by the \"child index\", e.g. the 2nd child of the root has index \"01\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea1baed9cacd8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:15:27.233999972Z",
     "start_time": "2024-01-10T12:14:51.527129818Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m      8\u001b[0m     random_points[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m      9\u001b[0m         [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(space[d][\u001b[38;5;241m0\u001b[39m], space[d][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ktree\u001b[38;5;241m.\u001b[39mdim)]\n\u001b[0;32m     10\u001b[0m     )\n\u001b[1;32m---> 11\u001b[0m st_acc \u001b[38;5;241m=\u001b[39m \u001b[43mktree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_critic_accuracies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(st_acc)\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\Documents\\Code\\GeoCluster\\src\\k_tree_ls.py:377\u001b[0m, in \u001b[0;36mKtree.get_critic_accuracies\u001b[1;34m(self, query_points)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node\u001b[38;5;241m.\u001b[39misLeaf():\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query_point \u001b[38;5;129;01min\u001b[39;00m query_points:\n\u001b[1;32m--> 377\u001b[0m         predicted_z \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m         z \u001b[38;5;241m=\u001b[39m predicted_z\u001b[38;5;241m.\u001b[39margmax()\n\u001b[0;32m    379\u001b[0m         predicted_nn \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mchildren[z]\u001b[38;5;241m.\u001b[39mquery(query_point)\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\Documents\\Code\\GeoCluster\\src\\models.py:536\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 536\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\panagiotis.rigas\\.conda\\envs\\Pytorch_main\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 30\n",
    "random_points = torch.zeros(n, ktree.dim)\n",
    "space = ktree.root.get_bounding_box()\n",
    "for i in range(n):\n",
    "    random_points[i] = torch.tensor(\n",
    "        [np.random.uniform(space[d][0], space[d][1]) for d in range(ktree.dim)]\n",
    "    )\n",
    "st_acc = ktree.get_critic_accuracies(random_points)\n",
    "\n",
    "print(st_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3257e5c8e338d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:30:38.730110485Z",
     "start_time": "2024-01-10T12:15:27.231556357Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc.random_queries(ktree, n=300, times=4, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7a43636b8a00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T12:34:12.270819369Z",
     "start_time": "2024-01-10T12:30:38.730141466Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc.serialised_queries(ktree, n=300, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
